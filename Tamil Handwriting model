{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":58786,"databundleVersionId":6350438,"sourceType":"competition"}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy==1.22.4\n!pip uninstall tensorflow-io -y\n!pip install tensorflow-io\n!pip install --upgrade numpy scipy tensorflow tensorflow-io","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:54:15.068136Z","iopub.execute_input":"2025-07-16T08:54:15.068774Z","iopub.status.idle":"2025-07-16T08:56:01.911162Z","shell.execute_reply.started":"2025-07-16T08:54:15.068743Z","shell.execute_reply":"2025-07-16T08:56:01.910023Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.22.4\n  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Uninstalling numpy-1.23.5:\n      Successfully uninstalled numpy-1.23.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nchex 0.1.81 requires numpy>=1.25.0, but you have numpy 1.22.4 which is incompatible.\ncudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.22.4 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.1 which is incompatible.\nraft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.22.4\nFound existing installation: tensorflow-io 0.31.0\nUninstalling tensorflow-io-0.31.0:\n  Successfully uninstalled tensorflow-io-0.31.0\nCollecting tensorflow-io\n  Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow-io-gcs-filesystem==0.37.1 (from tensorflow-io)\n  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorflow-io-gcs-filesystem, tensorflow-io\n  Attempting uninstall: tensorflow-io-gcs-filesystem\n    Found existing installation: tensorflow-io-gcs-filesystem 0.31.0\n    Uninstalling tensorflow-io-gcs-filesystem-0.31.0:\n      Successfully uninstalled tensorflow-io-gcs-filesystem-0.31.0\nSuccessfully installed tensorflow-io-0.37.1 tensorflow-io-gcs-filesystem-0.37.1\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.22.4)\nCollecting numpy\n  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.1)\nCollecting scipy\n  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\nCollecting tensorflow\n  Downloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-io in /opt/conda/lib/python3.10/site-packages (0.37.1)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nCollecting flatbuffers>=24.3.25 (from tensorflow)\n  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.31.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.6.3)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nCollecting tensorboard~=2.19.0 (from tensorflow)\n  Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting keras>=3.5.0 (from tensorflow)\n  Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting numpy\n  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting h5py>=3.11.0 (from tensorflow)\n  Downloading h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n  Downloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.4.2)\nCollecting namex (from keras>=3.5.0->tensorflow)\n  Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\nCollecting optree (from keras>=3.5.0->tensorflow)\n  Downloading optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (405 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.8/405.8 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.5.7)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (2.3.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\nInstalling collected packages: namex, flatbuffers, optree, numpy, tensorboard, scipy, ml-dtypes, h5py, keras, tensorflow\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 23.5.26\n    Uninstalling flatbuffers-23.5.26:\n      Successfully uninstalled flatbuffers-23.5.26\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.22.4\n    Uninstalling numpy-1.22.4:\n      Successfully uninstalled numpy-1.22.4\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.12.3\n    Uninstalling tensorboard-2.12.3:\n      Successfully uninstalled tensorboard-2.12.3\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.11.1\n    Uninstalling scipy-1.11.1:\n      Successfully uninstalled scipy-1.11.1\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.9.0\n    Uninstalling h5py-3.9.0:\n      Successfully uninstalled h5py-3.9.0\n  Attempting uninstall: keras\n    Found existing installation: keras 2.12.0\n    Uninstalling keras-2.12.0:\n      Successfully uninstalled keras-2.12.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.12.0\n    Uninstalling tensorflow-2.12.0:\n      Successfully uninstalled tensorflow-2.12.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 2.1.3 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbqplot 0.12.39 requires numpy<2.0.0,>=1.10.4, but you have numpy 2.1.3 which is incompatible.\ncudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ncupy 12.1.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.3 which is incompatible.\ndask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nibis-framework 6.0.0 requires numpy<2,>=1, but you have numpy 2.1.3 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nnumba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 2.1.3 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 2.1.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.15.3 which is incompatible.\nraft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ntensorflow-decision-forests 1.4.0 requires tensorflow~=2.12.0, but you have tensorflow 2.19.0 which is incompatible.\ntensorflow-text 2.12.1 requires tensorflow<2.13,>=2.12.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.19.0 which is incompatible.\ntensorflow-transform 0.14.0 requires numpy<2,>=1.16, but you have numpy 2.1.3 which is incompatible.\nydata-profiling 4.3.1 requires numpy<1.24,>=1.16.0, but you have numpy 2.1.3 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.15.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed flatbuffers-25.2.10 h5py-3.14.0 keras-3.10.0 ml-dtypes-0.5.1 namex-0.1.0 numpy-1.25.0 optree-0.16.0 scipy-1.15.3 tensorboard-2.19.0 tensorflow-2.19.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nimport os\nimport random\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:56:12.059953Z","iopub.execute_input":"2025-07-16T08:56:12.060602Z","iopub.status.idle":"2025-07-16T08:56:12.066293Z","shell.execute_reply.started":"2025-07-16T08:56:12.060557Z","shell.execute_reply":"2025-07-16T08:56:12.065313Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class TamilHandwritingRecognizer:\n    def __init__(self, img_height=64, img_width=256, max_length=32):\n        self.img_height = img_height\n        self.img_width = img_width\n        self.max_length = max_length\n        \n        # Tamil character set (simplified - expand as needed)\n        self.tamil_chars = [\n            'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'ஔ',\n            'க', 'ங', 'ச', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ப', 'ம', 'ய', 'ர', 'ல', 'வ', 'ழ', 'ள', 'ற', 'ன',\n            'ா', 'ி', 'ீ', 'ு', 'ூ', 'ெ', 'ே', 'ை', 'ொ', 'ோ', 'ௌ', '்',\n            ' ', '<PAD>', '<START>', '<END>'\n        ]\n        \n        self.char_to_idx = {char: idx for idx, char in enumerate(self.tamil_chars)}\n        self.idx_to_char = {idx: char for idx, char in enumerate(self.tamil_chars)}\n        self.vocab_size = len(self.tamil_chars)\n        \n        self.model = None\n\n\n    def build_model(self):\n        \"\"\"Build CNN + LSTM model for handwriting recognition\"\"\"\n        \n        # Input layer\n        input_img = layers.Input(shape=(self.img_height, self.img_width, 1), name='image_input')\n        \n        # CNN Feature Extraction\n        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n        x = layers.MaxPooling2D((2, 2))(x)\n        x = layers.BatchNormalization()(x)\n        \n        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n        x = layers.MaxPooling2D((2, 2))(x)\n        x = layers.BatchNormalization()(x)\n        \n        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n        x = layers.MaxPooling2D((2, 2))(x)\n        x = layers.BatchNormalization()(x)\n        \n        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n        x = layers.MaxPooling2D((2, 2))(x)\n        x = layers.BatchNormalization()(x)\n        \n        # Reshape for RNN\n        new_shape = ((self.img_width // 16), (self.img_height // 16) * 256)\n        x = layers.Reshape(target_shape=new_shape)(x)\n        x = layers.Dense(64, activation='relu')(x)\n        \n        # RNN layers\n        x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n        \n        # Output layer\n        output = layers.Dense(self.vocab_size, activation='softmax', name='output')(x)\n        \n        # Create model\n        self.model = models.Model(inputs=input_img, outputs=output)\n        \n        # Compile model\n        self.model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return self.model\n\n\n    def preprocess_image(self, image_path):\n        \"\"\"Preprocess image for model input\"\"\"\n        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n        \n        # Resize image\n        img = cv2.resize(img, (self.img_width, self.img_height))\n        \n        # Normalize\n        img = img.astype(np.float32) / 255.0\n        \n        # Add channel dimension\n        img = np.expand_dims(img, axis=-1)\n        \n        return img\n\n\n    def encode_text(self, text):\n        \"\"\"Encode text to sequence of indices\"\"\"\n        encoded = [self.char_to_idx.get(char, self.char_to_idx['<PAD>']) for char in text]\n        encoded = pad_sequences([encoded], maxlen=self.max_length, padding='post')[0]\n        return to_categorical(encoded, num_classes=self.vocab_size)\n\n\n    def decode_prediction(self, prediction):\n        \"\"\"Decode model prediction to text\"\"\"\n        predicted_indices = np.argmax(prediction, axis=-1)\n        decoded_text = ''.join([self.idx_to_char[idx] for idx in predicted_indices])\n        \n        # Remove padding and special tokens\n        decoded_text = decoded_text.replace('<PAD>', '').replace('<START>', '').replace('<END>', '')\n        \n        return decoded_text.strip()\n\n\n    def train(self, train_images, train_labels, validation_split=0.2, epochs=50, batch_size=32):\n        \"\"\"Train the model\"\"\"\n        \n        # Split data\n        X_train, X_val, y_train, y_val = train_test_split(\n            train_images, train_labels, test_size=validation_split, random_state=42\n        )\n        \n        # Callbacks\n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n            tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),\n            tf.keras.callbacks.ModelCheckpoint('tamil_handwriting_best.h5', save_best_only=True)\n        ]\n        \n        # Train model\n        history = self.model.fit(\n            X_train, y_train,\n            validation_data=(X_val, y_val),\n            epochs=epochs,\n            batch_size=batch_size,\n            callbacks=callbacks,\n            verbose=1\n        )\n        \n        return history\n\n    def predict(self, image):\n        \"\"\"Predict text from image\"\"\"\n        if len(image.shape) == 3:\n            image = np.expand_dims(image, axis=0)\n        \n        prediction = self.model.predict(image)\n        decoded_text = self.decode_prediction(prediction[0])\n        \n        # Calculate confidence\n        confidence = np.mean(np.max(prediction[0], axis=-1))\n        \n        return decoded_text, confidence\n\n\n# Data preparation functions\n    def prepare_tamil_dataset(data_path):\n        labels_df = pd.read_csv(os.path.join(data_path, 'labels.csv'))\n    \n        images = []\n        labels = []\n    \n        recognizer = TamilHandwritingRecognizer()\n    \n        for _, row in labels_df.iterrows():\n            image_path = os.path.join(data_path, 'images', row['filename'])\n        \n            if os.path.exists(image_path):\n            # Preprocess image\n                img = recognizer.preprocess_image(image_path)\n                images.append(img)\n            \n            # Encode label\n                encoded_label = recognizer.encode_text(row['text'])\n                labels.append(encoded_label)\n    \n        return np.array(images), np.array(labels)\n\n\n\n    def augment_image(img):\n        \"\"\"Apply random augmentation: rotation, shift, zoom, brightness, flips\"\"\"\n        img = tf.image.random_brightness(img, max_delta=0.2)\n        img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        # Random rotation\n        angle = random.uniform(-0.15, 0.15)  # radians\n        img_rotated = tf.image.rot90(img, k=int(angle / (tf.pi/2))) \n        return img\n\n    def evaluate_character_level(model, X, y_true, recognizer):\n        y_pred = model.predict(X)\n        y_pred_indices = np.argmax(y_pred, axis=-1)\n        y_true_indices = np.argmax(y_true, axis=-1)\n        acc = accuracy_score(y_true_indices, y_pred_indices)\n        cm = confusion_matrix(y_true_indices, y_pred_indices)\n        print(f\"Character-level accuracy: {acc:.4f}\")\n        return acc, cm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:56:16.620375Z","iopub.execute_input":"2025-07-16T08:56:16.620742Z","iopub.status.idle":"2025-07-16T08:56:16.643931Z","shell.execute_reply.started":"2025-07-16T08:56:16.620711Z","shell.execute_reply":"2025-07-16T08:56:16.642995Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def prepare_tamil_dataset_kaggle(data_path, recognizer, augment=False):\n    train_csv = os.path.join(data_path, \"train.csv\")\n    img_dir = os.path.join(data_path, \"Train-Kaggle\",\"Train-Kaggle\")\n    df = pd.read_csv(train_csv)\n\n    images = []\n    labels = []\n    skipped = 0\n    for idx, row in df.iterrows():\n        img_path = os.path.join(img_dir, row['ID'])\n        label_idx = row['Class Label']\n        # Data cleaning: skip if file missing or label out of range\n        if not os.path.exists(img_path):\n            skipped += 1\n            continue\n        if not (0 <= label_idx < len(recognizer.tamil_chars)):\n            skipped += 1\n            continue\n        try:\n            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n            if img is None:\n                skipped += 1\n                continue\n            img = cv2.resize(img, (recognizer.img_width, recognizer.img_height))\n            img = img.astype(np.float32) / 255.0\n            img = np.expand_dims(img, axis=-1)\n            if augment:\n                img = augment_image(img)\n            images.append(img)\n                # One-hot encode label\n            label = recognizer.char_to_idx[recognizer.tamil_chars[label_idx]]\n            label = tf.keras.utils.to_categorical(label, num_classes=recognizer.vocab_size)\n            labels.append(label)\n        except Exception:\n            skipped += 1\n            continue\n    print(f\"Loaded {len(images)} samples, skipped {skipped} due to errors.\")\n    return np.array(images), np.array(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:57:15.795265Z","iopub.execute_input":"2025-07-16T08:57:15.796117Z","iopub.status.idle":"2025-07-16T08:57:15.803759Z","shell.execute_reply.started":"2025-07-16T08:57:15.796085Z","shell.execute_reply":"2025-07-16T08:57:15.802819Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Training script\nif __name__ == \"__main__\":\n    # Initialize model\n    recognizer = TamilHandwritingRecognizer()\n    model = recognizer.build_model()\n    \n    print(\"Model Summary:\")\n    model.summary()\n    \n    # Example usage in your notebook:\n    data_path = \"/kaggle/input/tamil-hwcr\"\n    recognizer = TamilHandwritingRecognizer( img_height=64, img_width=256, max_length=32)\n    X_train, y_train = prepare_tamil_dataset_kaggle(data_path, recognizer, augment=True)\n# Now you can train as before:\n    history = recognizer.train(X_train, y_train, epochs=100)\n\n    # Load and prepare data\n    # Download Tamil handwriting dataset from:\n    # https://www.kaggle.com/datasets/tamil-handwriting-recognition\n    # Or create custom dataset\n    \n    data_path = \"/kaggle/input/tamil-hwcr\"\n    X_train, y_train = prepare_tamil_dataset(data_path)\n    \n    # Train model\n    history = recognizer.train(X_train, y_train, epochs=100)\n    \n    # Save model\n    model.save('tamil_handwriting_model.h5')\n    \n    print(\"Tamil Handwriting Recognition Model Ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:57:21.128158Z","iopub.execute_input":"2025-07-16T08:57:21.128484Z","iopub.status.idle":"2025-07-16T09:01:03.905452Z","shell.execute_reply.started":"2025-07-16T08:57:21.128458Z","shell.execute_reply":"2025-07-16T09:01:03.904426Z"}},"outputs":[{"name":"stdout","text":"Model Summary:\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n image_input (InputLayer)    [(None, 64, 256, 1)]      0         \n                                                                 \n conv2d (Conv2D)             (None, 64, 256, 32)       320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 32, 128, 32)      0         \n )                                                               \n                                                                 \n batch_normalization (BatchN  (None, 32, 128, 32)      128       \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 32, 128, 64)       18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 16, 64, 64)       0         \n 2D)                                                             \n                                                                 \n batch_normalization_1 (Batc  (None, 16, 64, 64)       256       \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 16, 64, 128)       73856     \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 8, 32, 128)       0         \n 2D)                                                             \n                                                                 \n batch_normalization_2 (Batc  (None, 8, 32, 128)       512       \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 8, 32, 256)        295168    \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 4, 16, 256)       0         \n 2D)                                                             \n                                                                 \n batch_normalization_3 (Batc  (None, 4, 16, 256)       1024      \n hNormalization)                                                 \n                                                                 \n reshape (Reshape)           (None, 16, 1024)          0         \n                                                                 \n dense (Dense)               (None, 16, 64)            65600     \n                                                                 \n bidirectional (Bidirectiona  (None, 16, 256)          197632    \n l)                                                              \n                                                                 \n bidirectional_1 (Bidirectio  (None, 16, 128)          164352    \n nal)                                                            \n                                                                 \n output (Dense)              (None, 16, 46)            5934      \n                                                                 \n=================================================================\nTotal params: 823,278\nTrainable params: 822,318\nNon-trainable params: 960\n_________________________________________________________________\nLoaded 0 samples, skipped 62870 due to errors.\nTamil Handwriting Recognition Model Ready!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}